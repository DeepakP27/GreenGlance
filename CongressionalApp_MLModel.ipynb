{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Kaggle API\n",
        "!pip install kaggle\n",
        "\n",
        "# Import the necessary library and upload kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()  # This will prompt you to upload kaggle.json\n",
        "\n",
        "# Create the Kaggle directory and move the kaggle.json file\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset (this downloads a ZIP file)\n",
        "!kaggle datasets download -d humansintheloop/recycling-dataset\n",
        "\n",
        "# Unzip the downloaded file\n",
        "!unzip recycling-dataset.zip -d /content/\n"
      ],
      "metadata": {
        "id": "uj4biaIyMUL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYNHrLXvMOke"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Paths to folders (adjust as needed)\n",
        "json_folder = '/content/ds0/ann'\n",
        "image_folder = '/content/ds0/img'\n",
        "\n",
        "# Initialize lists to store data\n",
        "image_data = []\n",
        "labels = []\n",
        "\n",
        "# Helper function to get labels from JSON files\n",
        "def get_label_from_json(json_file):\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        for obj in data['objects']:\n",
        "            for tag in obj['tags']:\n",
        "                if tag['name'] == 'Material':\n",
        "                    return tag['value']\n",
        "    return None\n",
        "\n",
        "# Process images and their corresponding labels\n",
        "def process_images_and_labels(json_folder, image_folder):\n",
        "    for json_filename in os.listdir(json_folder):\n",
        "        if json_filename.endswith('.json'):\n",
        "            json_path = os.path.join(json_folder, json_filename)\n",
        "\n",
        "            # Get the label (Material)\n",
        "            label = get_label_from_json(json_path)\n",
        "            if label is None:\n",
        "                continue  # Skip if no label is found\n",
        "\n",
        "            # Get the corresponding image filename (remove \".json\" to get the image file name)\n",
        "            image_filename = json_filename[:-5]  # Remove the last 5 characters ('.json')\n",
        "            image_path = os.path.join(image_folder, image_filename)\n",
        "\n",
        "            # Check if the image file exists\n",
        "            if os.path.exists(image_path):\n",
        "                print(f\"Processing image: {image_path}\")\n",
        "\n",
        "                # Load and process the image\n",
        "                image = load_img(image_path, target_size=(224, 224))\n",
        "                image = img_to_array(image) / 255.0  # Normalize image\n",
        "\n",
        "                # Append image and label\n",
        "                image_data.append(image)\n",
        "                labels.append(label)\n",
        "            else:\n",
        "                print(f\"Image not found for {json_filename}: {image_path}\")\n",
        "\n",
        "# Run the processing\n",
        "process_images_and_labels(json_folder, image_folder)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X = np.array(image_data)\n",
        "y = np.array(labels)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Check Lengths\n",
        "len(X)\n",
        "len(y)\n",
        "\n",
        "#Find the Unique Labels\n",
        "unique_labels = np.unique(y)\n",
        "print(\"Unique labels:\", unique_labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the training labels\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Transform the test labels\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Display the mappings\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(\"Label Mapping:\", label_mapping)"
      ],
      "metadata": {
        "id": "QOnsdqhmMXg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "'aluminum', 'celluloseacetate', 'glass', 'metal', 'paper', 'plastic', 'polystyrene', 'rubber', 'tetrapak', 'textile'\n",
        "# Count of images per class before removal\n",
        "current_counts = {\n",
        "    0: 322,\n",
        "    1: 16,\n",
        "    2: 186,\n",
        "    3: 18,\n",
        "    4: 596,\n",
        "    5: 1083,\n",
        "    6: 32,\n",
        "    7: 8,\n",
        "    8: 144,\n",
        "    9: 51\n",
        "}\n",
        "\n",
        "# Specify how many samples to remove from class 5\n",
        "samples_to_remove = 600\n",
        "\n",
        "# Find the indices of class 5\n",
        "class_5_indices = np.where(y_train_encoded == 5)[0]\n",
        "\n",
        "# Check if we have enough samples to remove\n",
        "if len(class_5_indices) < samples_to_remove:\n",
        "    print(f\"Not enough samples to remove from class 5. Available: {len(class_5_indices)}, Requested: {samples_to_remove}\")\n",
        "else:\n",
        "    # Randomly select indices to remove\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    indices_to_remove = np.random.choice(class_5_indices, size=samples_to_remove, replace=False)\n",
        "\n",
        "    # Create masks to keep only the indices not selected for removal\n",
        "    mask = np.isin(np.arange(len(y_train_encoded)), indices_to_remove, invert=True)\n",
        "\n",
        "    # Filter out the removed samples\n",
        "    X_train_filtered = X_train[mask]\n",
        "    y_train_filtered = y_train_encoded[mask]\n",
        "\n",
        "# Check the new counts after removal\n",
        "new_counts = np.unique(y_train_filtered, return_counts=True)\n",
        "print(\"New counts after removing samples from class 5:\")\n",
        "for label, count in zip(new_counts[0], new_counts[1]):\n",
        "    print(f\"Label {label}: {count}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mStYfOB4MbLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "# Load EfficientNetB0 model with pre-trained weights\n",
        "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "# Create the new model\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(len(unique_labels), activation='softmax')  # Assuming you have 10 classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Now you can fit your model using X_augmented and y_augmented\n",
        "history = model.fit(X_train_filtered, y_train_filtered, validation_data=(X_test, y_test_encoded), epochs=5, batch_size=32)\n",
        "\n",
        "#Evaluate Accuracy\n",
        "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "print(f'Test accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "wPNqgEOPMfHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:100]:  # Freeze the first 100 layers\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continue training\n",
        "history = model.fit(X_train_filtered, y_train_filtered, validation_data=(X_test, y_test_encoded), epochs=200, batch_size=32)\n",
        "\n"
      ],
      "metadata": {
        "id": "ejp3B5CPMgoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to visualize predictions\n",
        "def visualize_predictions(X_test, y_train_encoded, model, label_encoder, num_images=10):\n",
        "    # Make predictions on the test set\n",
        "    predictions = model.predict(X_train)\n",
        "\n",
        "    # Get the predicted classes by taking the argmax\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Select a random sample of images to display\n",
        "    random_indices = np.random.choice(20, size=num_images, replace=False)\n",
        "\n",
        "    # Create a plot\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, index in enumerate(random_indices):\n",
        "        plt.subplot(2, num_images // 2, i + 1)\n",
        "        plt.imshow(X_train[index])\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Get the actual and predicted labels\n",
        "        actual_label = y_train_encoded[index]\n",
        "        predicted_label = predicted_classes[index]\n",
        "\n",
        "        # Decode the labels back to original names\n",
        "        actual_label_name = label_encoder.inverse_transform([actual_label])[0]\n",
        "        predicted_label_name = label_encoder.inverse_transform([predicted_label])[0]\n",
        "\n",
        "        # Display the actual and predicted labels\n",
        "        plt.title(f\"Actual: {actual_label_name}\\nPredicted: {predicted_label_name}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to visualize predictions\n",
        "visualize_predictions(X_train, y_train_encoded, model, label_encoder, num_images=10)\n"
      ],
      "metadata": {
        "id": "_vTkw0syMiMK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}